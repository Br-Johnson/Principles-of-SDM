---
title: "Principles of SDM"
format: 
  docx:
    reference-doc: scidata-template.docx
bibliography: references.bib
csl: harvard.csl
execute:
  echo: false
  warning: false
---

Working title: Practical application of salmon data mobilization principles

# Overview of Paper Objectives

-   Focus on NE Pacific.

-   Use salmon as exemplar but indicate relevancy to other fisheries or ecosystem based survey data.

-   Focus is on communicating some basic principles for SDM

-   Create a roadmap (practical steps) to applying SDM principles for individuals and salmon data holding organizations looking to modernizing their data infrastructure or create FAIR data

    -   Reccommend specific platforms, technologies, templates, and processes.

-   Apply the roadmap to a case-study (CAP) to modernize a data system, mobilize a collection of well-curated data, and demonstrate new capabilities in discovering, integrating, and analyzing newly mobilized data.

    -   Create a 'toy example' of leveraging existing marine data already mobilized with newly mobilized salmon data. Provide R and Python scripts.

# Outline

Section 1: SDM Principles

Section 2: Applying SDM Principles to Solve Challenges Faced by Individual Scientist and Organizationally

Section 3: Case Study: SDM Principles Applied to Coordinated Assessment Program Data?

-   Success for section 3: Functional cases that others can apply. Toy example using freshwater and marine data.

## Important questions/points raised in email and discussion:

Here are some comments and points raised by each contributor thus far that align with and inform the overall vision of this work. Feel free to add more points here!

Jen:

-   "In our community, we don't really see adoption of existing standards such as Darwin Core but if that were to change, maybe it would make it easier for the 'federating' concept to work. If so, is there something we can do to make this change happen?"
-   "What changes are needed to enable data citations? metadata (including protocol) citations?"
-   "none of the state agencies and Tribes in the Columbia Basin who are definitely producing a lot of data make the Top 10 (salmon data producers). What could be done to change this figure in say, 5 years?"

Nancy:

-   What is preventing data from being discoverable by larger international organizations?
-   Recommend how to make PSMFC data discoverable
-   Use data sysytems of different scales and make recommendations to make them discoverable
-   Showcase how organizations have made their data discoverable

Tom:

-   Focus on how to achieve conceptual overview of SDM outlined in Diack et al
-   Overarching premise: Salmon recovery needs a shift towards ecosystem thinking and corresponding capacity to bring data and knowledge to bear at both broad and fine scales.
-   "the future is in harnessing and making sense distributed data collections."

Scott:

-   Challenge is to implement SDM
-   Need strategic plan for how to: - share datasets and code - implement standards for datasets - track provenance
-   Focus on rewards to end users
-   Engage early adopters for case studies
-   Think about providing template data dictionaries, changelogs, readme files

Tim:

-   Need examples. Not more theoretical guidelines
-   **Use cases**, public data set that demonstrate the applied values
-   Have the principles have the road map and then show how they are applied
-   Discoverability is but the first step
-   Cater toward salmon data producers
-   Narrow the geographic scope

# Literature Review

@satterthwaite2023 :

-   Inability to reproduce statistically significant results in fisheries stock assessments may be a result of p-hacking

@grossman2023 :

-   "For the purposes here, we view a *data commons* as a software platform that co-locates: 1) data, 2) cloud-based computing infrastructure, and 3) software applications, tools and services to create a governed resource for managing, analyzing, and sharing data with a community^"^

-   "A related concept is a data mesh (also known as a data ecosystem). A *data mesh* is a collection of data commons, cloud-based computational resources, and other cloud-based resources that interoperate using a common set of core software services and a hybrid governance model."

-   "Although the large and increasing amount of data being generated in biology, medicine and healthcare is well documented, the amount of well-curated, harmonized data is often not noted. This creates a"data gap" that impedes research. Data commons are designed in part to close this gap and allow research communities to create curated, harmonized data sets to accelerate their research."

-   "Successful commons curate and harmonize the data and produce data products of broad interest to the community. It's time consuming, expensive, and labor intensive to curate and harmonize data, by much of the value of data commons is centralizing this work so that it can be done once instead of many times by each group that needs the data."

@jenkins2023 **Reproducibility in ecology and evolution: Minimum standards for data and code**

> 1.  Detailed metadata with a README file
>
> 2.  Organized and clearly labeled data tables and files.
>
> 3.  Clearly outlined steps for data processing as described in the associated study.
>
> 4.  If bespoke scripts, analysis, or modeling methods were used, all associated programming scripts, software, and code required to run any analyses used in the study.
>
> 5.  Clear and consistent file naming, avoiding long names, spaces, and special characters
>
> 6.  References to other data, where applicable. Primary data should be included, either via repository upload or in the Appendix (provide DOIs). Secondary data users should cite the original data resources and respect all the conditions of data (re)use set by the data creators or managers
>
> 7.  Data should be stored in an open and re-usable format (e.g., .csv or .txt files for tables, not PDFs, and scripts in the original software format, not .txt).
>
> 8.  Clearly stated license under which the data are distributed (e.g., Creative Commons).

"In general, the choice of repository is up to the author, but we advise a field-specific repository. Note that some standardized types of data have specialized repositories (e.g., GenBank for sequencing data and Movebank for tracking data). Although popular, GitHub is not an acceptable repository for code, as it does not assign DOIs; in contrast, repositories such as Zenodo serve this purpose for code. GitHub's version control system, while useful in software development, is counterproductive with respect to reproducibility."

Read:

-   Poisot, T., Bruneau, A., Gonzalez, A., Gravel, D., & Peres-Neto, P. R. (2019). Ecological data should not Be so hard to find and reuse. *Trends in Ecology and Evolution*, **34**, 494-- 496.

@powers2018 **Open science, reproducibility, and transparency in ecology**

"Observations may be highly dependent on spatial and temporal context, making them very difficult to reproduce, but computational reproducibility can still be achieved."

"Computational reproducibility often refers to the ability to produce equivalent analytical outcomes from the same data set using the same code and software as the original study."

Check out RData Tracker: https://github.com/End-to-end-provenance/RDataTracker

# Introduction

There exists a mountain of data about salmon, their environment, and abundance yet scientists and managers struggle to understand why or predict when certain populations of salmon decline. Part of the challenge is that salmon have a complicated life history where they are exposed to numerous environments throughout their lives. Some parts of their lives are easier to observe than others and some environments are more complex than others. The challenge is to determine what observations have the most predictive power in each life phase, and then put all those data together to glean a complete and predictive history of the conditions encountered by specific salmon stocks.

However, because salmon cross arbitrary municipal, provincial, and national borders as well as habitat management areas (marine vs. freshwater) we lack a coordinated approach to aggregating data. Data often are collected using bespoke standards, stored locally, and not shared. Data management is not a trivial task. Therefore, any coordinated approach needs to be lightweight and flexible such that the barrier to adherence is surmountable by individual biologists and fall in line with the basic tenets of FAIR data.

The key to mobilizing salmon data doesn't neccesarily lie in creating new technologies, new databases, or bespoke data services. Rather, it rests on effectively harnessing and integrating existing ones, ensuring they are user-friendly, supporting their broad adoption, and raising awareness about how to contribute to and use them to effectively address organizational mandates. Adhering to practical principles and adopting modern data-servicing platforms and technologies will propel us towards the fourth paradigm of science: data-intensive science.

Success hinges on assisting agencies to meet their data publishing mandates. To achieve this, we must address challenges faced by individual biologists and data managers one at a time, providing solutions rooted in practicality and principles that are underpinned by well-adopted global best-practices, and aligning these with existing national and institutional mandates. By adopting the fundamentals outlined below, the salmon research community will be well-positioned to effectively manage, integrate, and harmonize the vast amount of data being produced to understand, predict and manage changes to salmon ecosystems.

# Salmon Data Mobilization Principles

1.  Publish Data and Metadata
2.  Respect Indigenous Data Sovereignty
3.  Create an Output Management Plan for Your Project
4.  Use Persistent Identifiers for Research Outputs
5.  Publish Protocols
6.  Adopt Domain-specific Data and Metadata Standards
7.  Promote, Incentivize and Require Data Publishing
8.  Promote and Support Reproducible Workflows
9.  Cite Reused Data

# A Roadmap to Applying SDM Principles for Organizations and Individual Scientists

Here we describe details of each principle and how each can be addressed by two different user-types: 1) Individual scientists with little to no organizational support for data publication and; 2) Salmon data holding organizations looking to publish FAIR @wilkinson2016 data or modernize their data infrastructure.

TODO: Expand on key concepts and terms used in descriptions here first so any one principle can be read and understood after reading this paragraph. Ie. define FAIR data, metadata, persistent identifiers, reproducibility. Define what is meant by salmon data (ie. spanning individual fish measurements and counts all the way to environmental conditions thought to affect salmon). Describe how their is currently no complete and widely accepted salmon data dictionary.

### **1. Publish Data and Metadata**

Data are a first-class research artifact and an organizational asset that scientists are increasingly being rewarded for by publishing. Through the process of publishing data, a metadata record is often created. A metadata record is usually a web-page in a data catalogue that provides a summary and description of the dataset using specific metadata fields and a link to download the data. A standard .xml file that structures metadata in a machine readable way is also provided in a metadata record.

Biologists need clear guidelines, tools and support to publish data and metadata. Institutions can provide clear mandates and resources to publish data and metadata. Academic journals can mandate data publication and citation for manuscripts[@cousijn2018].

\
*Individuals*

Individuals who are publishing a peer-reviewed journal article may choose to submit the data accompanying the paper to a generalist repository. This is an important practice that archives data for posterity and allows a greater degree of trust in the paper's results by reproducing and auditing the analyses. Recommended generalist repositories include [Zenodo](https://zenodo.org/), [Figshare](https://figshare.com/), [Dryad](https://datadryad.org/). By publishing a dataset to one of these repositories authors will receive a Digital Object Identifier (DOI) for their dataset and making the data findable, accessible, reusable and preserved as a first-class research artifact.

Note that these generalist repositories don't enforce any community-derived data standards that would esnure data are interoperable with other datasets. For datasets that are stand-alone time series data of an important metric, or suite of related metrics, individual scientists ought to consider publishing data to a domain-specific repository that enforces some data standardization which is covered in principle six: "Adopt Domain-specific Data and Metadata Standards".

*Organizations*

Organizations looking to publish data have a number of options available to publish data depending on requirements and capacity. For large organizations with dedicated software developers looking to implement custom metadata profiles and bespoke data publishing processes, hosting a data catalogue such as [GeoNetwork](https://www.geonetwork-opensource.org/) or a [Comprehensive Knowledge Archive Network (CKAN) Catalogue](https://ckan.org/) are both excellent options. These repositories both have nice interfaces for users to find data and most importantly can be configured to be harvested and indexed by third parties such as google datasets search, or other data catalogues that may crawl your catalogue looking for specific metadata to harvest and include in their own collections.

A great real world example of a Geonetwork catalogue is the [Strait of Georgia Data Centre](https://sogdatacentre.ca/) used by the Pacific Salmon Foundation. A few examples of a CKAN catalogue used for salmon data include the [Government of Canada's Open Data Portal](https://open.canada.ca/en/open-data), the [US Government's Open Data](https://data.gov/), and the [Canadian Integrated Ocean Observing System Data Portal](https://catalogue.cioos.ca/.).

Organizations not requiring a high degree of metadata customization or lack resources for a dedicated software developer to self-host a catalogue and archive data may wish to rely on third-party data and metadata publishing platforms. Several generalist repositories offer Institutional accounts that can be customized with institutional branding or customized metadata schemas for a nominal fee, which is almost certainly faster and more cost effective than hiring a dedicated developer. Dryad Data Repository offers the most comprehensive and up to date features supporting both DataCite and schema.org metadata schemas and is Core Trust Seal certified [@coretrustsealstandardsandcertificationboard2022], ensuring the repository adheres to standards and best-practices across the data management life cyle. To compare features of some of the most popular generalist repositories check out The General Repository Comparison Chart [@stall2023].

### **2. Respect Indigenous Data Sovereignty.**

The rights of indigenous peoples and nations to govern the collection, ownership, and application of their own data is a critical realization on the path to truth and reconciliation. Develop relationships with the nations on whose land you plan to work, practice community-engaged research[@sabatello2022], and apply the CARE principles[@carroll2020].

### **3. Create an Output Management Plan for Your Project**

A robust Output Management Plan is crucial for the effective management of the various outputs from scientific investigations. These are often referred to as Data Management Plans, but cover not only the management of data but facilitate a discussion around what outputs a scientific project will produce. Investigators can not only plan where and how they will store, share, and standardize data but also identify planned papers, reports, posters, presentations and datasets. Once those planned outputs are completed, scientists can identify those outputs as completed in their plan and provide persistent identifiers to where the outputs are published. This provides an excellent way for researchers to tie together various research outputs and offers funders a mechanism to review outputs against funded proposals.

Fisheries and Oceans Canada already prescribes this requirement ([**https://www.dfo-mpo.gc.ca/about-notre-sujet/publications/science/datapolicy-politiquedonnees/index-eng.html#6-5**](https://www.dfo-mpo.gc.ca/about-notre-sujet/publications/science/datapolicy-politiquedonnees/index-eng.html#6-5)). The Government of Canada's Tri-agency Research Data Management Policy and the Canadian Association of Research Libraries support DMP Assistant tool can aid in the creation of DMPs. However, we reccommend the California Digital Libraries DMP Tool (https://dmptool.org) which has the same code base but better features. (More on that to follow, including getting a DOI for a DMP to link outputs to a project plan)

### **4. Use Persistent Identifiers for Research Outputs**

Use persistent Identifiers (PIDs) to link together research elements and ensure they can be found far into the future. Mint Digital Object Identifiers (DOIs) for datasets or every other type of research output. Using [Open Research and Contributor ID (ORCIDs)](https://orcid.org/) to disambiguate individuals, and [Research Organization Registry (ROR)](https://ror.org/) for organizations can ensure precise, persistent and identification.

Scientific Organizations can join a regional [DataCite Consortium](https://support.datacite.org/v1.3/docs/datacite-consortia) if they wish to mint their own DOIs at a discounted rate. This offers a high degree of customization for institutions that would develop their own applications to archive or catalogue data using DataCite's Application Programming Interface. This can work especially well if insitutions are hosting their own data catlogue and data archive. Datacite also offers a graphical user interface ([DataCite Fabrica](https://doi.datacite.org/)) to mint DOIs that only requires filling out a form to create a DOI. While this method can be time intensive their is a low barrier to entry. Keep in mind there still needs to be a landing page created for the DOI to point users to, as well as an archive location for the data.

There are however, numerous pre-built solutions that can create a landing page, archive data, and mint a DOI organizations or rely on free services such as [zenodo](https://zenodo.org/). Moreover, the nascent [PID graph](https://doi.org/10.1016/j.patter.2020.100180) permits building custom applications or data visualizations that map research networks, connect researchers to projects, and summarize scholarly outputs from various organizations. There is massive infrastructure built to support scholarly bibliometrics, data discovery, linking, citation and data re-use using Digital Object Identifiers. This has to be a key component of salmon data mobilization.

### **5. Publish Protocols**

The publication of protocols ensures uniformity in data collection, analysis, and the ability to interpret whether data are fit for your purpose. Several platforms exist to publish protocols for which a Digital Object Identifier can be used: [Nature Portfolio's Protocol Exchange](https://protocolexchange.researchsquare.com/), [protocol.io](https://www.protocols.io/plans/academia), or simply using GitHub to host versions of protocols (DOIs can be assigned via the [Zenodo Integration](https://docs.github.com/en/repositories/archiving-a-github-repository/referencing-and-citing-content)). Protocols are not exempt from needing a DOI to ensure posterity and linking to articles, and datasets via DOI metadata (`relatedIdentifier` field specifically).

### **6. Adopt Domain-specific Data and Metadata Standards**

Implementing domain-specific data and metadata standards, such as Darwin Core for biological data and Climate Forecast Conventions for oceanographic data, ensures uniformity and compatibility across data sets. For primary biodiversity data, the Canadian Journal of Aquatic Fisheries Sciences *already* strongly advocates for all species distribution records to be deposited in publicly accessible databases like the Global Biodiversity Information Facility (GBIF) ([**www.gbif.org**](http://www.gbif.org/)), and the Ocean Biogeographic Information System (OBIS, [**http://www.iobis.org/**](http://www.iobis.org/)) for marine biodiversity data.

There is a need for a salmon-metrics specific controlled vocabulary or ontology. The [Salmon Ontology produced by NCEAS](https://bioportal.bioontology.org/ontologies/SALMON) is a good starting point, but needs community input, review, and refinement.

### **7. Promote and Incentivize Data Publishing**:

Encourage the scientific community to publish their data by offering incentives, rewards, and institutional recognition. Promote scientists publishing highly re-used data.

### **8. Promote and Support Reproducible Workflows**

Implement practices that ensure reproducibility of scientific workflows. This can be achieved by versioning data, code, and computational environments, and using workflow management systems that ensure the same computational procedure will yield the same result over time. This fosters trust, enables validation, promotes the reuse of scientific workflows, and hastens scientific progress. Researchers should provide clear and comprehensive documentation of their workflows, including code, data, and tools used, to support reproducibility by others in the scientific community. Institutions should provide training on these emergent skills and best-practices.

GitHub can support reproducible science [@braga2023]

### **9. Cite Reused Data**

Promote the reuse of data, leveraging the vast amount of existing information for new research and reduce duplication of efforts. Any reused data should be properly cited using its digital object identifier (DOI). This practice will ensure due acknowledgement to the original data creators and promote transparency, ethical data usage, and credit for the data provider. Progressive academic journals are already taking this approach following 'a data citation roadmap for scientific publishers' [@cousijn2018a]

# Case Study

Consider whether this should be a separate paper?

TODO: Chose a case study that demonstrates how to modernize an organizations data infrastructure and/or mobilize a specific data asset.

Proposed example: [Smolt to Adult Returns from Streamnet's Coordinated Assessment Program HLI.](https://www.streamnet.org/data/hli/?index=2&perpage=10&hli=SAR) Steps: 1) Assess dataset collection against SDM principles outlined, 2) Address relevant deficiencies: get a DOI for datasets (via GBIF?) and protocols, create an output management plan for this data mobilization project (dmptool.org), include recommended citation as column in data file, include DOI as column in data file. Assess where/how to standardize data. Before committing to strategy: investigate feasibility of transforming data to darwin core, requesting/creating new controlled vocabulary terms in NERC Vocabulary Server and publishing dataset on GBIF. If that works: visualize data downloads and citations mediated by GBIF.

Whatever the case study, the focus will be on modernizing infrastructure by adding some steps to reach SDM goals (not reinvent infrastructure). Generate a toy example of analyzing newly mobilized data integrated with some marine data using API and reproducible methods.

# Conclusion

The process of aggregating and understanding salmon data has historically been fraught with difficulties due to the complicated life history of the species and the geographical and political boundaries that often hamper a coordinated approach. The need of the hour is a simplified, flexible, and coordinated strategy that is easy to implement and adheres to the FAIR principles. The effective mobilization of salmon data does not hinge on the creation of new technologies. Rather, the emphasis should be on the active communication, coordination, and implementation of existing technologies, ensuring they are easy to use and their use is mandated. The principles outlined and demonstrated here present a holistic roadmap towards this objective. With concerted efforts and strategic implementation of these proposed strategies, salmon science may be able to enter the fourth scientific paradigm---data intensive scientific discoveries.

### References
